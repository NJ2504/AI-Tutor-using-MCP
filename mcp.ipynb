{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88060430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from typing import Generator, List\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65609a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Server cannot start.\")\n",
    "\n",
    "client = OpenAI(api_key = openai_api_key)\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244aba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATION_LEVELS = {\n",
    "    1: \"like I'm 5 years old\",\n",
    "    2: \"like I'm 10 years old\",\n",
    "    3: \"like a high school student\",\n",
    "    4: \"like a college student\",\n",
    "    5: \"like an expert in the field\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1‑5). If 1, explanation would be like we are talking to a 5 year old and if 5, explanation would be technical and complex.\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1‑5).\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0935713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, compression_ratio: float = 0.3) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream a summary of *text* compressed to roughly *compression_ratio* length.\n",
    "\n",
    "    *compression_ratio* should be between 0.1 and 0.8.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        yield \"Error: text cannot be blank.\"\n",
    "        return\n",
    "    ratio = max(0.1, min(compression_ratio, 0.8))\n",
    "    system_prompt = (\n",
    "        \"You are a world‑class summarizer. Reduce the following text to about \"\n",
    "        f\"{int(ratio*100)}% of its original length while preserving key ideas.\"\n",
    "    )\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.5,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f00234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flashcards(topic: str, num_cards: int = 5) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream *num_cards* Q/A flashcards for *topic* in JSON lines format.\"\"\"\n",
    "    if num_cards < 1 or num_cards > 20:\n",
    "        yield \"Error: num_cards must be between 1 and 20.\"\n",
    "        return\n",
    "    if not topic.strip():\n",
    "        yield \"Error: topic cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI that generates study flashcards. \"\n",
    "        'Return each flashcard on its own line as JSON: {\"q\": <question>, \"a\": <answer>}'\n",
    "    )\n",
    "    user_prompt = f\"Create {num_cards} flashcards about {topic}.\"\n",
    "\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.8,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz_me(topic: str, level: int = 3, num_questions: int = 5) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream a quiz with numbered Qs then reveal answers after all questions.\"\"\"\n",
    "    if num_questions < 1 or num_questions > 15:\n",
    "        yield \"Error: num_questions must be between 1 and 15.\"\n",
    "        return\n",
    "    if not topic.strip():\n",
    "        yield \"Error: topic cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"at an intermediate level\")\n",
    "    system_prompt = (\n",
    "        \"You are an AI quiz master. Generate a quiz of multiple‑choice questions \"\n",
    "        f\"about {topic} {level_desc}. Number the questions. After listing all Qs, \"\n",
    "        \"add an \\nANSWER KEY section with the correct options.\"\n",
    "    )\n",
    "\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}],\n",
    "        stream = True,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d688890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# AI Tutor MCP Toolkit – Demo Console\")\n",
    "        with gr.Tab(\"Explain Concept\"):\n",
    "            q = gr.Textbox(label=\"Concept / Question\")\n",
    "            lvl = gr.Slider(1, 5, value=3, step=1, label=\"Explanation Level\")\n",
    "            out1 = gr.Markdown()\n",
    "            gr.Button(\"Explain\").click(explain_concept, inputs=[q, lvl], outputs=out1)\n",
    "        with gr.Tab(\"Summarize Text\"):\n",
    "            txt = gr.Textbox(lines=8, label=\"Long Text\")\n",
    "            ratio = gr.Slider(0.1, 0.8, value=0.3, step=0.05, label=\"Compression Ratio\")\n",
    "            out2 = gr.Markdown()\n",
    "            gr.Button(\"Summarize\").click(summarize_text, inputs=[txt, ratio], outputs=out2)\n",
    "        with gr.Tab(\"Flashcards\"):\n",
    "            topic_fc = gr.Textbox(label=\"Topic\")\n",
    "            n_fc = gr.Slider(1, 20, value=5, step=1, label=\"# Cards\")\n",
    "            out3 = gr.Markdown()\n",
    "            gr.Button(\"Generate\").click(generate_flashcards, inputs=[topic_fc, n_fc], outputs=out3)\n",
    "        with gr.Tab(\"Quiz Me\"):\n",
    "            topic_q = gr.Textbox(label=\"Topic\")\n",
    "            lvl_q = gr.Slider(1, 5, value=3, step=1, label=\"Difficulty Level\")\n",
    "            n_q = gr.Slider(1, 15, value=5, step=1, label=\"# Questions\")\n",
    "            out4 = gr.Markdown()\n",
    "            gr.Button(\"Start Quiz\").click(quiz_me, inputs=[topic_q, lvl_q, n_q], outputs=out4)\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting AI Tutor MCP Toolkit on port 7860…\")\n",
    "    build_demo().launch(server_name = \"0.0.0.0\", mcp_server = True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
