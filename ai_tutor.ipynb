{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9435c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests  \n",
    "import httpx  \n",
    "import json  \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, Image  \n",
    "from openai import OpenAI  \n",
    "\n",
    "from PIL import Image\n",
    "import asyncio, pathlib\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372481f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_BASE = \"http://localhost:7860/gradio_api/mcp/sse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c839659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.mcp import MCPServerSse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13381377",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_tool = MCPServerSse({\n",
    "    \"name\": \"AI Tutor\",\n",
    "    \"url\": MCP_BASE,\n",
    "    \"timeout\": 30,\n",
    "    \"client_session_timeout_seconds\":60\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpx.Client() \n",
    "def fetch_schema(server_url):\n",
    "    \"\"\"Fetches and parses the MCP schema from a server.\"\"\"\n",
    "    \n",
    "    schema_url = server_url.replace(\"/sse\", \"/schema\")\n",
    "    print(f\"Fetching schema from: {schema_url}\")\n",
    "    \n",
    "    response = client.get(schema_url, timeout = 10)  # Add a timeout\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "    schema_data = response.json()\n",
    "    print(\"Schema fetched successfully!\")\n",
    "    return schema_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0653380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fetching AI Tutor Schema ---\")\n",
    "tutor_schema = fetch_schema(MCP_BASE)\n",
    "\n",
    "if tutor_schema:\n",
    "    print(\"\\nAI Tutor Schema Contents:\")\n",
    "    # Pretty print the JSON manifest\n",
    "    print(json.dumps(tutor_schema, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "agent = Agent(\n",
    "    name = \"Smart Assistant\",\n",
    "    instructions = \"\"\"\n",
    "    Context\n",
    "    -------\n",
    "    You are an AI assistant with access to an MCP server exposing **four streaming tools**:\n",
    "\n",
    "    1. **explain_concept**  \n",
    "    Arguments: { \"question\": <str>, \"level\": <int 1‑5> }  \n",
    "    • Streams an explanation of any concept at the requested depth.\n",
    "\n",
    "    2. **summarize_text**  \n",
    "    Arguments: { \"text\": <str>, \"compression_ratio\": <float 0.1‑0.8> }  \n",
    "    • Streams a concise summary ~compression_ratio × original length.\n",
    "\n",
    "    3. **generate_flashcards**  \n",
    "    Arguments: { \"topic\": <str>, \"num_cards\": <int 1‑20> }  \n",
    "    • Streams JSON‑lines flashcards: one card per line `{ \"q\":…, \"a\":… }`.\n",
    "\n",
    "    4. **quiz_me**  \n",
    "    Arguments: { \"topic\": <str>, \"level\": <int 1‑5>, \"num_questions\": <int 1‑15> }  \n",
    "    • Streams an MC‑question quiz, then an ANSWER KEY section.\n",
    "\n",
    "    Objective\n",
    "    ---------\n",
    "    Help users learn by:\n",
    "    • Explaining concepts at the depth they request.  \n",
    "    • Summarising long passages.  \n",
    "    • Generating flashcards for self‑study.  \n",
    "    • Quizzing them interactively.\n",
    "\n",
    "    How to respond\n",
    "    --------------\n",
    "    • For each user request, decide which tool (if any) fulfils it best.  \n",
    "    • Call the tool via MCP by returning *only* the JSON with `\"tool\"` and `\"arguments\"` (no extra text).  \n",
    "    • If a follow‑up conversation is needed (e.g., clarification), ask the user first.  \n",
    "    • If no tool fits, answer directly in plain language.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    User: “Explain quantum tunnelling like I’m 10.”  \n",
    "    → Call `explain_concept` with { \"question\": \"quantum tunnelling\", \"level\": 2 }\n",
    "\n",
    "    User: “Summarise this article to 20 %.” + <article text>  \n",
    "    → Call `summarize_text` with { \"text\": \"...\", \"compression_ratio\": 0.2 }\n",
    "\n",
    "    Chat capability\n",
    "    ---------------\n",
    "    After each tool call completes (streaming back to the user), remain in the chat loop ready for the next user turn.\n",
    "    \"\"\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    mcp_servers = [mcp_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await mcp_tool.connect()  \n",
    "result = None\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "        \n",
    "    if result is not None:\n",
    "        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": user_input}]\n",
    "    else:\n",
    "        new_input = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    print(\"\\nUser Input:\")\n",
    "    print_markdown(user_input)\n",
    "\n",
    "    \n",
    "   # This is the core AI agent execution step. It runs your agent with the new_input.\n",
    "\n",
    "    result = await Runner.run(starting_agent = agent, input = new_input)\n",
    "    print(\"\\nAssistant:\")\n",
    "    print_markdown(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result.to_input_list():\n",
    "    for key in i.keys():\n",
    "        if key == 'arguments':\n",
    "            print(\"Tool: \", i['name'])\n",
    "            print(\"Arguments: \", i['arguments'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
